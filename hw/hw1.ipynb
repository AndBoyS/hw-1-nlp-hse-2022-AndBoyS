{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание\n",
    "## Harry Potter and the Action Prediction Challenge from Natural Language\n",
    "\n",
    "*deadline*: 14 ноября 2022, 23:59\n",
    "\n",
    "В этом домашнем задании вы будете работать с корпусом Harry Potter and the Action Prediction Challenge. Корпус собран из фанфиков о Гарри Поттере и состоит из двух частей: 1) сырые тексты, 2) фрагменты текстов, описывающих ситуацию, в которой произнесено заклинание.\n",
    "\n",
    "Корпус описан в статье: https://arxiv.org/pdf/1905.11037.pdf\n",
    "\n",
    "David Vilares and Carlos Gómez-Rodríguez. Harry Potter and the Action Prediction Challenge from Natural Language. 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics. To appear.\n",
    "\n",
    "Код для сбора корпуса находится в репозитории: https://github.com/aghie/hpac . Корпус можно скачать по инструкции из этого репозитория, но для экономии времени авторы задания уже скачали и подготовили данные к работе. \n",
    "\n",
    "Ссылки на собранный корпус: \n",
    "* Сырые тексты:  https://www.dropbox.com/s/23xet9kvbqna1qs/hpac_raw.zip?dl=0\n",
    "* Токенизированные тексты в нижнем регистре: https://www.dropbox.com/s/gwfgmomdbetvdye/hpac_lower_tokenized.zip?dl=0\n",
    "* train-test-dev: https://www.dropbox.com/s/3vdz0mouvex8abd/hpac_splits.zip?dl=0\n",
    "\n",
    "Части 1, 2 задания должны быть выполнены на полных текстах (сырых или предобработанных -- на ваше усмотрение), Часть 3 – на разбиение на тестовое, отладочное и обучающее множества. Тестовое множество должно быть использовано исключительно для тестирования моделей, обучающее и отладочное – для выбора модели и параметров. \n",
    "\n",
    "В статье и репозитории вы найдете идеи, которые помогут вам выполнить домашнее задание. Их стоит воспринимать как руководство к действию, и не стоит их копировать и переиспользовать. Обученные модели использовать не нужно, код для их обучения можно использовать как подсказку. \n",
    "\n",
    "## ПРАВИЛА\n",
    "1. Домашнее задание выполняется в группе до 3-х человек.\n",
    "2. Домашнее задание оформляется в виде отчета либо в .pdf файле, либо ipython-тетрадке. \n",
    "3. Отчет должен содержать: нумерацию заданий и пунктов, которые вы выполнили, код решения, и понятное пошаговое описание того, что вы сделали. Отчет должен быть написан в академическом стиле, без излишнего использования сленга и с соблюдением норм русского языка.\n",
    "4. Не стоит копировать фрагменты лекций, статей и Википедии в ваш отчет.\n",
    "5. Отчеты, состоящие исключительно из кода, не будут проверены и будут автоматически оценены нулевой оценкой.\n",
    "6. Плагиат и любое недобросоветсное цитирование приводит к обнуление оценки. \n",
    "\n",
    "\n",
    "## Часть 1. [2 балла] Эксплоративный анализ \n",
    "1. Найдите топ-1000 слов по частоте без учета стоп-слов.\n",
    "2. Найдите топ-10 по частоте: имен, пар имя + фамилия, пар вида ''профессор'' + имя / фамилия. \n",
    "\n",
    "[бонус] Постройте тематическую модель по корпусу HPAC.\n",
    "\n",
    "[бонус] Найдите еще что-то интересное в корпусе (что-то специфичное для фанфиков или фентези-тематики)\n",
    "\n",
    "## Часть 2. [2 балла] Модели представления слов \n",
    "Обучите модель представления слов (word2vec, GloVe, fastText или любую другую) на материале корпуса HPAC.\n",
    "1. Продемонстрируйте, как работает поиск синонимов, ассоциаций, лишних слов в обученной модели. \n",
    "2. Визуализируйте топ-1000 слов по частоте без учета стоп-слов (п. 1.1) с помощью TSNE или UMAP (https://umap-learn.readthedocs.io).\n",
    "\n",
    "## Часть 3. [5 баллов] Классификация текстов\n",
    "Задача классификации формулируется так: данный фрагмент фанфика описывают какую-то ситуацию, которая предшествует произнесению заклинания. Требуется по тексту предсказать, какое именно заклинание будет произнесено. Таким образом, заклинание - это фактически метка класса. Основная мера качества – macro $F_1$.\n",
    "Обучите несколько классификаторов и сравните их между собой. Оцените качество классификаторов на частых и редких классах. Какие классы чаще всего оказываются перепутаны? Связаны ли ошибки со смыслом заклинаний?\n",
    "\n",
    "Используйте фрагменты из множества train для обучения, из множества dev для отладки, из множества test – для тестирования и получения итоговых результатов. \n",
    "\n",
    "1. [1 балл] Используйте fastText в качестве baseline-классификатора.\n",
    "2. [2 балла] Используйте сверточные  или реккурентные сети в качестве более продвинутого классификатора. Поэкспериментируйте с количеством и размерностью фильтров, используйте разные размеры окон, попробуйте использовать $k$-max pooling. \n",
    "3. [2 балла] Попробуйте расширить обучающее множество за счет аугментации данных. Если вам понадобится словарь синонимов, можно использовать WordNet (ниже вы найдете примеры).\n",
    "\n",
    "[бонус] Используйте результат max pooling'а как эмбеддинг входного текста. Визуализируйте эмбеддинги 500-1000 предложений из обучающего множества и изучите свойства получившегося пространства.\n",
    "\n",
    "[бонус] Используйте ваш любимый классификатор и любые (честные) способы повышения качества классификации и получите macro $F_1$ больше 0.5.\n",
    "\n",
    "## Часть 4. [1 балл] Итоги\n",
    "Напишите краткое резюме проделанной работы. Читали ли вы сами Гарри Поттера или фанфики о нем и помогло ли вам знание предметной области в выполнении домашнего задания?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\r\n",
      "/Users/user/Main/Study/NLP/hw-1-nlp-hse-2022-AndBoyS/hw/data\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "%cd data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-11-07 20:59:19--  https://www.dropbox.com/s/23xet9kvbqna1qs/hpac_raw.zip\r\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18\r\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: /s/raw/23xet9kvbqna1qs/hpac_raw.zip [following]\r\n",
      "--2022-11-07 20:59:23--  https://www.dropbox.com/s/raw/23xet9kvbqna1qs/hpac_raw.zip\r\n",
      "Reusing existing connection to www.dropbox.com:443.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com/cd/0/inline/BwX6_goqPJo1ECXTvh_cWpu92zklf6UlqQ6zzu9k6-c0lQFaFax-lxeRHu-R0Jy5mufEhXp3j717P-DzpLDcekkpXv6zTt7Jn3VRrE-0uX9lWrEE0mfNzpPpx9LFBiOEzs7MwKRgZ-eHacdOhTEY-b9KckcuKkYFfyXgMBfgrcwBTg/file# [following]\r\n",
      "--2022-11-07 20:59:24--  https://uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com/cd/0/inline/BwX6_goqPJo1ECXTvh_cWpu92zklf6UlqQ6zzu9k6-c0lQFaFax-lxeRHu-R0Jy5mufEhXp3j717P-DzpLDcekkpXv6zTt7Jn3VRrE-0uX9lWrEE0mfNzpPpx9LFBiOEzs7MwKRgZ-eHacdOhTEY-b9KckcuKkYFfyXgMBfgrcwBTg/file\r\n",
      "Resolving uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com (uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com)... 162.125.64.15\r\n",
      "Connecting to uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com (uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: /cd/0/inline2/BwWbamQ57pATCA2QaNvrZk1FMExLPYT1Dj0dM1zACVuQ5YSEIkW_odvPGwXJ9IKE6eOlBlKfuroKno3nUlZ4wf-hTmNUefl7gEHHjpZClCKiOPudoRtY5dCRUGSdT2p6elsIV9IrmX_YuWTruVm0GrmpkpAkODsmGtgHYz6jAWYcM2rsmezf6CFoViPT9OwNz-7f6YSDaPnEBrxMQH0btSN3LZ882HmvfzO3PskG9iIYVv2Uw6kJ26vFePdI0KtsCYfCZl95iy0J9a16PkY-lalO3MnIqPlG_vQLQCXuEQeV-7aSp-W87XacZSnaK81RHiCG4oI6KZszgooMkRt2wwwn5KW4f_4IuNdX2nfxvxJuvva2zKpygniEiY8Q0PWrj_j3s2O0yeA2NVjcpZpetwbAWkIE2hCCjk1n6RY8n22W8Q/file [following]\r\n",
      "--2022-11-07 20:59:46--  https://uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com/cd/0/inline2/BwWbamQ57pATCA2QaNvrZk1FMExLPYT1Dj0dM1zACVuQ5YSEIkW_odvPGwXJ9IKE6eOlBlKfuroKno3nUlZ4wf-hTmNUefl7gEHHjpZClCKiOPudoRtY5dCRUGSdT2p6elsIV9IrmX_YuWTruVm0GrmpkpAkODsmGtgHYz6jAWYcM2rsmezf6CFoViPT9OwNz-7f6YSDaPnEBrxMQH0btSN3LZ882HmvfzO3PskG9iIYVv2Uw6kJ26vFePdI0KtsCYfCZl95iy0J9a16PkY-lalO3MnIqPlG_vQLQCXuEQeV-7aSp-W87XacZSnaK81RHiCG4oI6KZszgooMkRt2wwwn5KW4f_4IuNdX2nfxvxJuvva2zKpygniEiY8Q0PWrj_j3s2O0yeA2NVjcpZpetwbAWkIE2hCCjk1n6RY8n22W8Q/file\r\n",
      "Reusing existing connection to uc9a2656e9b64c11d44ee9de006a.dl.dropboxusercontent.com:443.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 1018234025 (971M) [application/zip]\r\n",
      "Saving to: ‘hpac_raw.zip’\r\n",
      "\r\n",
      "hpac_raw.zip        100%[===================>] 971.06M  1.12MB/s    in 23m 10s \r\n",
      "\r\n",
      "2022-11-07 21:22:57 (715 KB/s) - ‘hpac_raw.zip’ saved [1018234025/1018234025]\r\n",
      "\r\n",
      "--2022-11-07 21:22:57--  https://www.dropbox.com/s/gwfgmomdbetvdye/hpac_lower_tokenized.zip\r\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18\r\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: /s/raw/gwfgmomdbetvdye/hpac_lower_tokenized.zip [following]\r\n",
      "--2022-11-07 21:22:58--  https://www.dropbox.com/s/raw/gwfgmomdbetvdye/hpac_lower_tokenized.zip\r\n",
      "Reusing existing connection to www.dropbox.com:443.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com/cd/0/inline/BwWCmPm2dxyNSU-lnD9T5JpMVahm-SMWoBBJ9dWiHR0_GujfU1wORVgK9_Mun2-jFpC10JUCgZumzYePPZ3m0KwnDYWXRn5qW1qFe2bfI11rH8oARCFX6jVXFWRuzUMpaX51lg3f_8nfEtgZfv645KmRwypCoUlqfYYV1SrOFgZLug/file# [following]\r\n",
      "--2022-11-07 21:22:59--  https://uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com/cd/0/inline/BwWCmPm2dxyNSU-lnD9T5JpMVahm-SMWoBBJ9dWiHR0_GujfU1wORVgK9_Mun2-jFpC10JUCgZumzYePPZ3m0KwnDYWXRn5qW1qFe2bfI11rH8oARCFX6jVXFWRuzUMpaX51lg3f_8nfEtgZfv645KmRwypCoUlqfYYV1SrOFgZLug/file\r\n",
      "Resolving uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com (uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com)... 162.125.64.15\r\n",
      "Connecting to uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com (uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: /cd/0/inline2/BwXA75GPhKHHfgshS-Q1YVUuYhneO0ndMIUNpVY4KQWtpIz-1sZuxcDrcOExRZHWTRCzAAdSH8SAlv8dvo3QA4dY7JwpCcMfEoKf2jB5Y5bdZ-oa5vIbBTMASi6aNAfBGUWPo2SsmU98I98oJwgc5dVwJaZ45Th-23WrdLDAcqSOFO6oaN70VTdgOLxEDwamdhkQ92d0yXmAY1g8G5SiTY8pYr_8SMKPWLnHUfvNOvuRxdYwsl_5FZ0pUyyeb6dj2MetLf48GLAqI62oqFJcmpel2j8WTWUosvaQyMrNIlcHksa8k0-h7jF_yC4Wl6tdsJrJs3u94S-2c9Rz-CseVq7CP-KRnNIHZJMx-9z17kf2pJnSVwnCjZyfr_jXX7s-BHVgJtL5HCXRYfgOrexS1_PTfu7-kHq08AN8em7LlJv9qw/file [following]\r\n",
      "--2022-11-07 21:23:00--  https://uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com/cd/0/inline2/BwXA75GPhKHHfgshS-Q1YVUuYhneO0ndMIUNpVY4KQWtpIz-1sZuxcDrcOExRZHWTRCzAAdSH8SAlv8dvo3QA4dY7JwpCcMfEoKf2jB5Y5bdZ-oa5vIbBTMASi6aNAfBGUWPo2SsmU98I98oJwgc5dVwJaZ45Th-23WrdLDAcqSOFO6oaN70VTdgOLxEDwamdhkQ92d0yXmAY1g8G5SiTY8pYr_8SMKPWLnHUfvNOvuRxdYwsl_5FZ0pUyyeb6dj2MetLf48GLAqI62oqFJcmpel2j8WTWUosvaQyMrNIlcHksa8k0-h7jF_yC4Wl6tdsJrJs3u94S-2c9Rz-CseVq7CP-KRnNIHZJMx-9z17kf2pJnSVwnCjZyfr_jXX7s-BHVgJtL5HCXRYfgOrexS1_PTfu7-kHq08AN8em7LlJv9qw/file\r\n",
      "Reusing existing connection to uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com:443.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 996528740 (950M) [application/zip]\r\n",
      "Saving to: ‘hpac_lower_tokenized.zip’\r\n",
      "\r\n",
      "hpac_lower_tokenize  99%[==================> ] 947.75M   487KB/s    in 23m 25s \r\n",
      "\r\n",
      "2022-11-07 21:46:26 (691 KB/s) - Connection closed at byte 993787904. Retrying.\r\n",
      "\r\n",
      "--2022-11-07 21:46:27--  (try: 2)  https://uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com/cd/0/inline2/BwXA75GPhKHHfgshS-Q1YVUuYhneO0ndMIUNpVY4KQWtpIz-1sZuxcDrcOExRZHWTRCzAAdSH8SAlv8dvo3QA4dY7JwpCcMfEoKf2jB5Y5bdZ-oa5vIbBTMASi6aNAfBGUWPo2SsmU98I98oJwgc5dVwJaZ45Th-23WrdLDAcqSOFO6oaN70VTdgOLxEDwamdhkQ92d0yXmAY1g8G5SiTY8pYr_8SMKPWLnHUfvNOvuRxdYwsl_5FZ0pUyyeb6dj2MetLf48GLAqI62oqFJcmpel2j8WTWUosvaQyMrNIlcHksa8k0-h7jF_yC4Wl6tdsJrJs3u94S-2c9Rz-CseVq7CP-KRnNIHZJMx-9z17kf2pJnSVwnCjZyfr_jXX7s-BHVgJtL5HCXRYfgOrexS1_PTfu7-kHq08AN8em7LlJv9qw/file\r\n",
      "Connecting to uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com (uc4ca054c1de2b8c05cb388efa91.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 206 Partial Content\r\n",
      "Length: 996528740 (950M), 2740836 (2.6M) remaining [application/zip]\r\n",
      "Saving to: ‘hpac_lower_tokenized.zip’\r\n",
      "\r\n",
      "hpac_lower_tokenize 100%[+++++++++++++++++++>] 950.36M   903KB/s    in 3.0s    \r\n",
      "\r\n",
      "2022-11-07 21:46:32 (903 KB/s) - ‘hpac_lower_tokenized.zip’ saved [996528740/996528740]\r\n",
      "\r\n",
      "--2022-11-07 21:46:32--  https://www.dropbox.com/s/3vdz0mouvex8abd/hpac_splits.zip\r\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.64.18\r\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: /s/raw/3vdz0mouvex8abd/hpac_splits.zip [following]\r\n",
      "--2022-11-07 21:46:36--  https://www.dropbox.com/s/raw/3vdz0mouvex8abd/hpac_splits.zip\r\n",
      "Reusing existing connection to www.dropbox.com:443.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: https://uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com/cd/0/inline/BwWUT3hrv_WgQPguHA7uNt5s3bOfAk6-p1mtO_0dRs9aNyjHGw919KuXeNc7oY_rrdQjhBLSv1CZpousig5U3y670N17zeWwkdzVi6KID75wd-1X-PivVVUbfxSjdJbudggTZQ5gdrVCxQlk_8BXTHSMqfwbiU4g1P5VBb08YoruCw/file# [following]\r\n",
      "--2022-11-07 21:46:36--  https://uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com/cd/0/inline/BwWUT3hrv_WgQPguHA7uNt5s3bOfAk6-p1mtO_0dRs9aNyjHGw919KuXeNc7oY_rrdQjhBLSv1CZpousig5U3y670N17zeWwkdzVi6KID75wd-1X-PivVVUbfxSjdJbudggTZQ5gdrVCxQlk_8BXTHSMqfwbiU4g1P5VBb08YoruCw/file\r\n",
      "Resolving uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com (uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com)... 162.125.64.15\r\n",
      "Connecting to uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com (uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 302 Found\r\n",
      "Location: /cd/0/inline2/BwX1RZANc7r_H2vfQ8TBZTMSoDcWrsgD8wcDxtnqWHBJdi8qajBddQD8EaA-dsfSflyoStkyM3KCYJrg1XCS67n5HttZrthzGEF2l2JbJWGbB2D-BwLPD_Jckmd8dM8hArz8tbu28EmyrKcTTF41d3JI9B3TQgOQXTd6XU_GC7j5WIxrVu2jP5Bd7Gn5H-Xs0vfmR1pc1AF6C6whb83Lw0lUWyRfzlj_yFW_mZDIaUvWWKxGqG4jWYmRqlA-yj1Gs-P61DFQjBYjwmg2eB9gUeuVpwfsZotQU5CX63-8oQLZQ1DpwVWi9sJKquwAlhQmlZSQaUwK_Twadmd7E7HY3CMeAaErL6rwpLcsNC_G_8KIB2nq3B1GT2KRY4F7wWG8LqTgqfavmE4Em6rYHtUDTSKWSioHJ8siRz6bbAi80kLwmw/file [following]\r\n",
      "--2022-11-07 21:46:44--  https://uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com/cd/0/inline2/BwX1RZANc7r_H2vfQ8TBZTMSoDcWrsgD8wcDxtnqWHBJdi8qajBddQD8EaA-dsfSflyoStkyM3KCYJrg1XCS67n5HttZrthzGEF2l2JbJWGbB2D-BwLPD_Jckmd8dM8hArz8tbu28EmyrKcTTF41d3JI9B3TQgOQXTd6XU_GC7j5WIxrVu2jP5Bd7Gn5H-Xs0vfmR1pc1AF6C6whb83Lw0lUWyRfzlj_yFW_mZDIaUvWWKxGqG4jWYmRqlA-yj1Gs-P61DFQjBYjwmg2eB9gUeuVpwfsZotQU5CX63-8oQLZQ1DpwVWi9sJKquwAlhQmlZSQaUwK_Twadmd7E7HY3CMeAaErL6rwpLcsNC_G_8KIB2nq3B1GT2KRY4F7wWG8LqTgqfavmE4Em6rYHtUDTSKWSioHJ8siRz6bbAi80kLwmw/file\r\n",
      "Reusing existing connection to uc11a57b9d130d6b1c8a7afed3bc.dl.dropboxusercontent.com:443.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 18284862 (17M) [application/zip]\r\n",
      "Saving to: ‘hpac_splits.zip’\r\n",
      "\r\n",
      "hpac_splits.zip     100%[===================>]  17.44M   806KB/s    in 32s     \r\n",
      "\r\n",
      "2022-11-07 21:47:16 (565 KB/s) - ‘hpac_splits.zip’ saved [18284862/18284862]\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://www.dropbox.com/s/23xet9kvbqna1qs/hpac_raw.zip\n",
    "!wget -nc https://www.dropbox.com/s/gwfgmomdbetvdye/hpac_lower_tokenized.zip\n",
    "!wget -nc https://www.dropbox.com/s/3vdz0mouvex8abd/hpac_splits.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Импортируем базовые библиотеки"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "import re\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import nltk\n",
    "# скачиваем модули для NLTK\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Данные\n",
    "\n",
    "Распакуем и переименуем данные для удобства"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Как папки будут называться в итоге\n",
    "raw_data_dir = Path('hpac_raw')\n",
    "tokenized_data_dir = Path('hpac_lower_tokenized')\n",
    "split_data_dir = Path('hpac_splits')\n",
    "\n",
    "if not raw_data_dir.exists():\n",
    "    !unzip hpac_raw\n",
    "if not tokenized_data_dir.exists():\n",
    "    !unzip hpac_lower_tokenized\n",
    "if not split_data_dir.exists():\n",
    "    !unzip hpac_splits\n",
    "shutil.rmtree('__MACOSX', ignore_errors=True)\n",
    "\n",
    "folder_mapping = {\n",
    "    'fanfiction_texts': raw_data_dir.name,\n",
    "    'hpac_source': tokenized_data_dir.name,\n",
    "    'hpac_corpus': split_data_dir.name,\n",
    "}\n",
    "\n",
    "for name, new_name in folder_mapping.items():\n",
    "    fp = Path(name)\n",
    "    if fp.exists():\n",
    "        fp.rename(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   36225\r\n"
     ]
    }
   ],
   "source": [
    "!ls hpac_lower_tokenized | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train, test, dev файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(split_data_dir / 'hpac_training_128.tsv',\n",
    "                 names=['target', 'text'],\n",
    "                 index_col=0,\n",
    "                 sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "                       target  \\\n7642954.0.676      RIDDIKULUS   \n10443333.0.5753    RIDDIKULUS   \n4703706.0.8690        STUPEFY   \n4593427.0.1815          ACCIO   \n4278446.0.2692   EXPELLIARMUS   \n\n                                                              text  \n7642954.0.676    were staring at her . she was up next to face ...  \n10443333.0.5753  that whole time . her first reaction , for whi...  \n4703706.0.8690   we watched his inglorious withdrawal together ...  \n4593427.0.1815   my wand , `` incendio . '' this wretched chill...  \n4278446.0.2692   already compared ours , they 're the same ever...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7642954.0.676</th>\n      <td>RIDDIKULUS</td>\n      <td>were staring at her . she was up next to face ...</td>\n    </tr>\n    <tr>\n      <th>10443333.0.5753</th>\n      <td>RIDDIKULUS</td>\n      <td>that whole time . her first reaction , for whi...</td>\n    </tr>\n    <tr>\n      <th>4703706.0.8690</th>\n      <td>STUPEFY</td>\n      <td>we watched his inglorious withdrawal together ...</td>\n    </tr>\n    <tr>\n      <th>4593427.0.1815</th>\n      <td>ACCIO</td>\n      <td>my wand , `` incendio . '' this wretched chill...</td>\n    </tr>\n    <tr>\n      <th>4278446.0.2692</th>\n      <td>EXPELLIARMUS</td>\n      <td>already compared ours , they 're the same ever...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "('RIDDIKULUS',\n \"were staring at her . she was up next to face the boggart in defense against the dark arts class . she was not scared , but what she was worried about was what had happened with lysander . she looked up at the boggart in front of her which had previously been a humongous spider . its eyes locked on her . before she could think of what frightened her , the spider transformed into lysander . he was dying . there were giggles coming from the male and female hufflepuff students . there was a smirk on lorcan 's face . `` lily help me '' i ca n't fail this class because of a secret love . lily lifted her wand and said , ``\")"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][0], df.iloc[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как использовать WordNet из nltk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[Synset('magic.n.01'), Synset('magic_trick.n.01'), Synset('charming.s.02')]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# слово -> множество синсетов (синонимов разных смыслов исходного слова)\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('magic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Lemma('magic_trick.n.01.magic_trick')"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим, что внутри одного синсета\n",
    "wn.synsets('magic')[1].lemmas()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'deception'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# возьмем лемму одного из слов из синсета\n",
    "wn.synsets('magic')[1].lemmas()[-1].name()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть 1. [2 балла] Эксплоративный анализ\n",
    "1. Найдите топ-1000 слов по частоте без учета стоп-слов."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Загрузим токенизированные тексты и уберем пунктуацию"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/36225 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0b03e759edc94fcf8886289549be7766"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Удаляем пунктуацию, не считая тире\n",
    "def read_text_no_punc(text_dir: Path):\n",
    "    return [re.sub(r'[^\\w\\s-]', '', fp.read_text())\n",
    "            for fp in tqdm(list(text_dir.glob('*')))]\n",
    "\n",
    "tokenized_texts = read_text_no_punc(tokenized_data_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.5 s, sys: 11.5 s, total: 15 s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count, Pool\n",
    "import utils\n",
    "\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield n number of striped chunks from l.\"\"\"\n",
    "    for i in range(0, n):\n",
    "        yield l[i::n]\n",
    "\n",
    "def get_word_counters_mp(texts: List[str]):\n",
    "\n",
    "    num_processes = cpu_count() - 1\n",
    "    texts = list(chunks(texts, num_processes))\n",
    "\n",
    "    with Pool(num_processes) as p:\n",
    "        counters = p.map(utils.get_word_counters, texts)\n",
    "\n",
    "    # Объединяем результаты счетчиков\n",
    "    res_counter = counters[0]\n",
    "    for counter in counters:\n",
    "        for name, val in counter.items():\n",
    "            res_counter[name] += val\n",
    "\n",
    "    return res_counter\n",
    "\n",
    "%time word_counter = get_word_counters_mp(tokenized_texts)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-1000 слов по встречаемости:\n"
     ]
    },
    {
     "data": {
      "text/plain": "[('nt', 4812279),\n ('harry', 4399595),\n ('said', 2481138),\n ('would', 2085120),\n ('hermione', 2007971),\n ('could', 1849050),\n ('back', 1532008),\n ('draco', 1530258),\n ('one', 1508594),\n ('like', 1375897),\n ('know', 1291608),\n ('eyes', 1129603),\n ('time', 1101650),\n ('ron', 988329),\n ('looked', 980838),\n ('--', 947821),\n ('get', 927184),\n ('asked', 910145),\n ('well', 857566),\n ('even', 841953),\n ('around', 841202),\n ('see', 815056),\n ('head', 800675),\n ('going', 785722),\n ('think', 783685),\n ('still', 757338),\n ('go', 730061),\n ('severus', 725841),\n ('way', 715868),\n ('face', 713731),\n ('room', 709709),\n ('ginny', 697241),\n ('hand', 693659),\n ('something', 683137),\n ('sirius', 681343),\n ('want', 671984),\n ('thought', 666287),\n ('-', 664534),\n ('potter', 661412),\n ('right', 658852),\n ('snape', 652693),\n ('away', 635975),\n ('much', 631241),\n ('look', 619460),\n ('two', 617429),\n ('never', 611382),\n ('knew', 575928),\n ('really', 574742),\n ('first', 564897),\n ('let', 562403),\n ('made', 560686),\n ('good', 548989),\n ('malfoy', 530107),\n ('little', 526146),\n ('wand', 525483),\n ('felt', 521250),\n ('dumbledore', 520505),\n ('turned', 518367),\n ('james', 507511),\n ('come', 504617),\n ('got', 494868),\n ('make', 485175),\n ('took', 479518),\n ('remus', 468106),\n ('though', 458306),\n ('sure', 453821),\n ('lily', 451692),\n ('say', 450852),\n ('door', 447817),\n ('tell', 445292),\n ('take', 444030),\n ('us', 440641),\n ('looking', 438548),\n ('dark', 436455),\n ('voice', 433860),\n ('last', 431674),\n ('voldemort', 430554),\n ('long', 428741),\n ('told', 425280),\n ('need', 424736),\n ('left', 418564),\n ('yes', 416412),\n ('man', 412130),\n ('wanted', 405244),\n ('anything', 401491),\n ('next', 391783),\n ('oh', 385930),\n ('came', 377119),\n ('nodded', 373162),\n ('love', 369793),\n ('moment', 369323),\n ('people', 366100),\n ('saw', 365705),\n ('another', 364001),\n ('things', 358916),\n ('went', 357360),\n ('hands', 355847),\n ('ca', 353006),\n ('help', 352652),\n ('day', 350730),\n ('enough', 350270),\n ('death', 347488),\n ('smiled', 343217),\n ('professor', 341897),\n ('year', 338443),\n ('mind', 338048),\n ('nothing', 337324),\n ('found', 336416),\n ('ever', 333015),\n ('boy', 331914),\n ('always', 328464),\n ('hair', 328100),\n ('find', 326905),\n ('bit', 324001),\n ('seemed', 322327),\n ('behind', 321870),\n ('hogwarts', 318441),\n ('thing', 314732),\n ('bed', 313700),\n ('trying', 311290),\n ('started', 308649),\n ('feel', 308428),\n ('put', 307474),\n ('since', 304042),\n ('life', 301910),\n ('house', 301903),\n ('night', 300398),\n ('heard', 299629),\n ('without', 291966),\n ('smile', 291722),\n ('black', 291618),\n ('better', 289607),\n ('years', 289605),\n ('-RRB-', 288686),\n ('gave', 287993),\n ('might', 285984),\n ('magic', 285951),\n ('weasley', 283927),\n ('side', 283608),\n ('everyone', 282205),\n ('father', 280962),\n ('sat', 279306),\n ('someone', 277541),\n ('began', 277474),\n ('walked', 275732),\n ('almost', 274751),\n ('finally', 271890),\n ('done', 271061),\n ('already', 270596),\n ('tried', 269094),\n ('place', 268108),\n ('every', 267187),\n ('stood', 266701),\n ('everything', 263405),\n ('-LRB-', 263287),\n ('friends', 263009),\n ('front', 261529),\n ('three', 261353),\n ('pulled', 260708),\n ('small', 260683),\n ('quickly', 260615),\n ('lord', 260472),\n ('also', 260237),\n ('course', 258438),\n ('keep', 257159),\n ('girl', 255790),\n ('body', 254556),\n ('best', 252870),\n ('towards', 250248),\n ('else', 244178),\n ('arms', 243555),\n ('neville', 243190),\n ('table', 242178),\n ('give', 240367),\n ('mean', 240271),\n ('family', 238685),\n ('work', 237965),\n ('sorry', 237883),\n ('please', 235168),\n ('albus', 234908),\n ('end', 234239),\n ('many', 234109),\n ('lucius', 233402),\n ('world', 233100),\n ('mother', 232185),\n ('school', 231786),\n ('great', 231176),\n ('old', 229847),\n ('together', 228533),\n ('new', 227256),\n ('stop', 226142),\n ('quite', 225947),\n ('happened', 225799),\n ('leave', 225498),\n ('maybe', 224125),\n ('replied', 224079),\n ('mouth', 222092),\n ('open', 221732),\n ('yet', 221027),\n ('soon', 221004),\n ('must', 220642),\n ('later', 217308),\n ('floor', 216472),\n ('getting', 215104),\n ('able', 214385),\n ('words', 212502),\n ('arm', 211448),\n ('name', 211274),\n ('friend', 209327),\n ('mr', 209116),\n ('least', 208747),\n ('lips', 208618),\n ('hard', 208114),\n ('actually', 206090),\n ('feeling', 205819),\n ('suddenly', 204878),\n ('anyone', 204445),\n ('seen', 204317),\n ('slowly', 203733),\n ('sighed', 201931),\n ('talk', 200253),\n ('luna', 199950),\n ('spell', 199210),\n ('george', 196328),\n ('shook', 196209),\n ('making', 195474),\n ('gone', 195012),\n ('rather', 193443),\n ('believe', 193201),\n ('probably', 192327),\n ('held', 191386),\n ('home', 190666),\n ('far', 190520),\n ('light', 189705),\n ('inside', 189642),\n ('granger', 189359),\n ('used', 189075),\n ('opened', 188166),\n ('wo', 187837),\n ('second', 187534),\n ('fred', 186935),\n ('rest', 186476),\n ('taking', 185310),\n ('stopped', 183282),\n ('hear', 182884),\n ('idea', 181958),\n ('rose', 181645),\n ('whispered', 181064),\n ('coming', 180776),\n ('tom', 180772),\n ('across', 180167),\n ('breath', 179860),\n ('care', 179574),\n ('try', 179376),\n ('thank', 179282),\n ('slightly', 177726),\n ('students', 177445),\n ('parents', 177365),\n ('yeah', 177311),\n ('called', 176312),\n ('needed', 176115),\n ('ask', 175544),\n ('close', 174289),\n ('laughed', 174180),\n ('may', 173952),\n ('watched', 173094),\n ('use', 173058),\n ('slytherin', 172396),\n ('continued', 171834),\n ('gryffindor', 171576),\n ('sitting', 171444),\n ('part', 170135),\n ('morning', 170011),\n ('blood', 169751),\n ('heart', 169660),\n ('feet', 169475),\n ('happy', 168818),\n ('lot', 167950),\n ('pain', 165672),\n ('book', 163942),\n ('remember', 163383),\n ('hall', 163252),\n ('muggle', 162968),\n ('okay', 162060),\n ('fact', 161895),\n ('past', 161670),\n ('reached', 160227),\n ('however', 159695),\n ('chapter', 159506),\n ('fine', 159368),\n ('stared', 159216),\n ('thinking', 157921),\n ('wrong', 157449),\n ('along', 157046),\n ('ministry', 157020),\n ('onto', 156612),\n ('alone', 156284),\n ('hope', 156229),\n ('shoulder', 155085),\n ('read', 154226),\n ('moved', 153905),\n ('wizard', 153767),\n ('set', 153515),\n ('standing', 152794),\n ('bad', 152661),\n ('either', 152369),\n ('chest', 151646),\n ('red', 151631),\n ('decided', 151249),\n ('fell', 150557),\n ('talking', 150419),\n ('minutes', 149436),\n ('dead', 149007),\n ('stay', 148940),\n ('ran', 148285),\n ('mcgonagall', 148035),\n ('young', 147870),\n ('understand', 147271),\n ('potion', 147196),\n ('hurt', 146599),\n ('fingers', 146546),\n ('caught', 145567),\n ('person', 145465),\n ('instead', 145273),\n ('kill', 145185),\n ('days', 144745),\n ('woman', 144414),\n ('potions', 144376),\n ('miss', 144330),\n ('point', 143742),\n ('air', 143144),\n ('others', 143001),\n ('kiss', 142471),\n ('ground', 142263),\n ('tonks', 142259),\n ('ready', 142096),\n ('tears', 141646),\n ('kept', 141512),\n ('matter', 140670),\n ('taken', 140258),\n ('times', 140255),\n ('son', 140202),\n ('wait', 139827),\n ('forward', 138438),\n ('order', 138368),\n ('deep', 138266),\n ('closed', 138220),\n ('noticed', 138038),\n ('curse', 136276),\n ('story', 135935),\n ('eaters', 135692),\n ('reason', 135178),\n ('kind', 134486),\n ('lost', 134472),\n ('raised', 134278),\n ('sound', 133178),\n ('full', 133147),\n ('magical', 132552),\n ('thanks', 132431),\n ('sleep', 132325),\n ('move', 132037),\n ('blaise', 130612),\n ('different', 130149),\n ('wall', 129766),\n ('whole', 129254),\n ('several', 128875),\n ('office', 128508),\n ('question', 127404),\n ('bellatrix', 126848),\n ('followed', 126770),\n ('start', 125946),\n ('half', 125134),\n ('word', 124984),\n ('large', 124917),\n ('turn', 124768),\n ('outside', 124156),\n ('perhaps', 124114),\n ('answer', 124039),\n ('anyway', 123542),\n ('holding', 122997),\n ('attention', 122959),\n ('class', 122950),\n ('chair', 122264),\n ('answered', 121698),\n ('hit', 121649),\n ('exactly', 121612),\n ('scorpius', 121317),\n ('completely', 121131),\n ('waiting', 121005),\n ('lupin', 119817),\n ('quietly', 119752),\n ('cold', 118699),\n ('mum', 118641),\n ('immediately', 118530),\n ('pansy', 118459),\n ('ago', 118198),\n ('spoke', 117828),\n ('longer', 117797),\n ('robes', 117725),\n ('saying', 117649),\n ('eye', 117635),\n ('minerva', 117244),\n ('today', 117062),\n ('leaving', 116893),\n ('stepped', 116244),\n ('watching', 116239),\n ('true', 115813),\n ('green', 115795),\n ('hold', 115250),\n ('seeing', 115219),\n ('surprised', 115024),\n ('master', 114911),\n ('child', 114762),\n ('staring', 114605),\n ('given', 114492),\n ('silence', 114107),\n ('dad', 113817),\n ('turning', 113482),\n ('nice', 113305),\n ('nearly', 113261),\n ('neck', 113046),\n ('softly', 112966),\n ('fire', 112762),\n ('headmaster', 112701),\n ('brought', 112458),\n ('meant', 112204),\n ('finished', 112151),\n ('simply', 112107),\n ('closer', 111700),\n ('witch', 111367),\n ('grabbed', 110550),\n ('four', 110103),\n ('hell', 109849),\n ('managed', 109500),\n ('grinned', 109394),\n ('show', 109338),\n ('charm', 108574),\n ('call', 108572),\n ('change', 108360),\n ('realized', 108259),\n ('bloody', 108248),\n ('war', 108201),\n ('top', 107886),\n ('narcissa', 107861),\n ('looks', 107683),\n ('says', 107175),\n ('running', 106973),\n ('quidditch', 106664),\n ('children', 106652),\n ('kissed', 106362),\n ('speak', 106103),\n ('common', 106075),\n ('killed', 105939),\n ('chance', 105888),\n ('mrs', 105833),\n ('shut', 105488),\n ('brother', 105252),\n ('skin', 104894),\n ('week', 104195),\n ('sent', 104095),\n ('pointed', 103047),\n ('alright', 102596),\n ('castle', 102536),\n ('rolled', 102377),\n ('met', 102352),\n ('added', 101972),\n ('returned', 101713),\n ('known', 101690),\n ('expression', 101602),\n ('muttered', 101319),\n ('boys', 101232),\n ('supposed', 101205),\n ('walking', 101135),\n ('molly', 101102),\n ('whatever', 101080),\n ('appeared', 100998),\n ('fight', 100866),\n ('knowing', 100751),\n ('hours', 100677),\n ('run', 100465),\n ('cast', 100436),\n ('leaned', 100298),\n ('girls', 99921),\n ('giving', 99721),\n ('happen', 99313),\n ('less', 99299),\n ('books', 98698),\n ('thoughts', 98628),\n ('upon', 98335),\n ('shrugged', 98335),\n ('pretty', 98116),\n ('water', 98098),\n ('guess', 97768),\n ('spells', 97725),\n ('letter', 97478),\n ('throat', 97299),\n ('sort', 97230),\n ('worry', 97138),\n ('pushed', 96814),\n ('gently', 96673),\n ('real', 96596),\n ('loved', 96063),\n ('big', 95632),\n ('hagrid', 95556),\n ('possible', 95481),\n ('stairs', 95459),\n ('surprise', 95265),\n ('couple', 95186),\n ('white', 94757),\n ('glanced', 94657),\n ('passed', 94492),\n ('meet', 94237),\n ('seem', 94068),\n ('beside', 93877),\n ('five', 93678),\n ('bill', 93639),\n ('fear', 93308),\n ('desk', 92923),\n ('become', 92786),\n ('placed', 92655),\n ('watch', 92465),\n ('stand', 92300),\n ('sit', 92193),\n ('dinner', 91814),\n ('power', 91738),\n ('smiling', 91606),\n ('corner', 91191),\n ('sense', 91053),\n ('although', 90789),\n ('toward', 90675),\n ('late', 90404),\n ('merlin', 90117),\n ('safe', 90093),\n ('bring', 89963),\n ('die', 89950),\n ('sir', 89872),\n ('stupid', 89511),\n ('walk', 89210),\n ('wizards', 88827),\n ('soft', 88416),\n ('baby', 88365),\n ('anymore', 88106),\n ('snapped', 87831),\n ('wondered', 87705),\n ('eater', 87576),\n ('kitchen', 87541),\n ('knows', 87393),\n ('agreed', 87361),\n ('quiet', 86969),\n ('wish', 86940),\n ('telling', 86524),\n ('changed', 86066),\n ('return', 85999),\n ('window', 85927),\n ('break', 85888),\n ('group', 85875),\n ('moving', 85585),\n ('laugh', 85548),\n ('angry', 85402),\n ('pulling', 85340),\n ('peter', 85286),\n ('plan', 84902),\n ('dropped', 84424),\n ('short', 84419),\n ('case', 84345),\n ('trust', 84305),\n ('near', 84302),\n ('free', 84219),\n ('charlie', 84175),\n ('shoulders', 84078),\n ('shot', 83936),\n ('explained', 83907),\n ('sister', 83389),\n ('gaze', 83275),\n ('reading', 83016),\n ('sight', 82959),\n ('live', 82705),\n ('barely', 82649),\n ('beautiful', 82043),\n ('died', 81860),\n ('especially', 81817),\n ('within', 81580),\n ('christmas', 81523),\n ('entire', 81500),\n ('figure', 81490),\n ('months', 81479),\n ('cut', 81442),\n ('worried', 81339),\n ('clear', 81308),\n ('spent', 81210),\n ('tonight', 81194),\n ('seat', 81167),\n ('memory', 80956),\n ('shaking', 80878),\n ('working', 80877),\n ('legs', 80756),\n ('shouted', 80738),\n ('blue', 80608),\n ('dear', 80384),\n ('stomach', 79995),\n ('frowned', 79937),\n ('living', 79821),\n ('conversation', 79767),\n ('fall', 79584),\n ('straight', 79056),\n ('allowed', 78959),\n ('tone', 78920),\n ('remembered', 78667),\n ('control', 78589),\n ('afraid', 78492),\n ('wizarding', 78291),\n ('stone', 78107),\n ('form', 77988),\n ('entered', 77923),\n ('picked', 77793),\n ('fun', 77792),\n ('glad', 77686),\n ('anger', 77625),\n ('broke', 77418),\n ('touch', 77415),\n ('hour', 76882),\n ('silent', 76691),\n ('truth', 76658),\n ('quick', 76605),\n ('grin', 76388),\n ('hey', 76180),\n ('nose', 76173),\n ('step', 76148),\n ('smirked', 76052),\n ('cried', 75990),\n ('laughing', 75888),\n ('meeting', 75726),\n ('certain', 75485),\n ('play', 75441),\n ('important', 75230),\n ('weeks', 75197),\n ('became', 75144),\n ('filled', 74791),\n ('auror', 74668),\n ('none', 74582),\n ('warm', 74558),\n ('using', 74416),\n ('food', 74183),\n ('fleur', 74107),\n ('older', 73897),\n ('empty', 73808),\n ('percy', 73736),\n ('threw', 73629),\n ('team', 73290),\n ('hate', 72844),\n ('expected', 72840),\n ('yelled', 72804),\n ('cheek', 72460),\n ('worse', 72171),\n ('breakfast', 72158),\n ('ones', 72100),\n ('problem', 72091),\n ('memories', 72040),\n ('clearly', 71899),\n ('cloak', 71879),\n ('waited', 71682),\n ('perfect', 71384),\n ('shock', 70972),\n ('daphne', 70893),\n ('arrived', 70806),\n ('liked', 70798),\n ('obviously', 70774),\n ('teddy', 70735),\n ('certainly', 70402),\n ('sometimes', 70337),\n ('alive', 70286),\n ('owl', 70008),\n ('note', 69870),\n ('wife', 69710),\n ('summer', 69600),\n ('carefully', 69570),\n ('despite', 69289),\n ('madam', 69197),\n ('except', 69081),\n ('ten', 69068),\n ('job', 68982),\n ('information', 68931),\n ('forced', 68889),\n ('loud', 68832),\n ('gotten', 68813),\n ('riddle', 68715),\n ('wrapped', 68684),\n ('tomorrow', 68462),\n ('clothes', 68456),\n ('deal', 68395),\n ('glass', 68294),\n ('library', 68266),\n ('calm', 68060),\n ('strong', 68053),\n ('tea', 67786),\n ('strange', 67773),\n ('trouble', 67651),\n ('confused', 67581),\n ('worked', 67473),\n ('battle', 67351),\n ('ear', 67187),\n ('twins', 67185),\n ('evening', 67173),\n ('mine', 66656),\n ('easy', 66573),\n ('normal', 66441),\n ('seems', 66366),\n ('parchment', 66349),\n ('apparently', 66344),\n ('lay', 66317),\n ('minute', 66287),\n ('daughter', 66254),\n ('wide', 66174),\n ('moments', 66058),\n ('makes', 66055),\n ('paused', 66037),\n ('middle', 65725),\n ('tired', 65237),\n ('broom', 65108),\n ('notice', 64968),\n ('somehow', 64941),\n ('high', 64886),\n ('suppose', 64876),\n ('led', 64869),\n ('cup', 64136),\n ('promise', 64116),\n ('lying', 64091),\n ('secret', 64010),\n ('usual', 64001),\n ('piece', 63695),\n ('questions', 63562),\n ('chuckled', 63554),\n ('broken', 63533),\n ('men', 63480),\n ('line', 63349),\n ('tongue', 63341),\n ('sounded', 63270),\n ('arthur', 63106),\n ('mate', 63001),\n ('doubt', 62825),\n ('means', 62780),\n ('attack', 62719),\n ('familiar', 62694),\n ('save', 62644),\n ('pale', 62643),\n ('wonder', 62622),\n ('helped', 62572),\n ('muggles', 62533),\n ('finger', 62321),\n ('explain', 62311),\n ('hissed', 62171),\n ('bright', 62039),\n ('jumped', 61991),\n ('situation', 61919),\n ('minister', 61898),\n ('uncle', 61895),\n ('charms', 61855),\n ('gasped', 61783),\n ('pressed', 61759),\n ('hospital', 61736),\n ('lifted', 61676),\n ('manor', 61670),\n ('aurors', 61496),\n ('whether', 61282),\n ('game', 61269),\n ('glared', 61199),\n ('position', 61027),\n ('headed', 60796),\n ('bella', 60572),\n ('seconds', 60571),\n ('dobby', 60511),\n ('wants', 60310),\n ('catch', 60261),\n ('mark', 60066),\n ('kingsley', 60005),\n ('moody', 59972),\n ('often', 59867),\n ('asking', 59824),\n ('shall', 59768),\n ('fast', 59750),\n ('teeth', 59397),\n ('pomfrey', 59249),\n ('third', 59214),\n ('early', 59173),\n ('listen', 59141),\n ('asleep', 59077),\n ('wearing', 59072),\n ('neither', 58993),\n ('bag', 58631),\n ('letting', 58423),\n ('elf', 58333),\n ('forget', 58323),\n ('besides', 58288),\n ('following', 58279),\n ('write', 58272),\n ('pull', 58209),\n ('covered', 58090),\n ('sigh', 58038),\n ('screamed', 58010),\n ('follow', 58009),\n ('truly', 57958),\n ('hoped', 57834),\n ('beginning', 57758),\n ('keeping', 57690),\n ('shirt', 57591),\n ('student', 57559),\n ('flew', 57542),\n ('fighting', 57392),\n ('wondering', 57167),\n ('likely', 57155),\n ('guys', 57147),\n ('easily', 57053),\n ('hated', 56973),\n ('direction', 56965),\n ('playing', 56964),\n ('usually', 56906),\n ('eyebrow', 56853),\n ('learn', 56826),\n ('somewhere', 56816),\n ('future', 56792),\n ('ended', 56770),\n ('starting', 56696),\n ('eat', 56515),\n ('crying', 56499),\n ('enjoy', 56263),\n ('edge', 56184),\n ('lip', 56136),\n ('al', 56116),\n ('husband', 56082),\n ('flying', 56009),\n ('earlier', 55511),\n ('ah', 55471),\n ('azkaban', 55261),\n ('offered', 55252),\n ('soul', 55239),\n ('cheeks', 55088),\n ('hoping', 55060),\n ('cry', 55029),\n ('train', 54895),\n ('falling', 54855),\n ('damn', 54822),\n ('send', 54782),\n ('knees', 54729),\n ('shocked', 54713),\n ('powerful', 54686),\n ('money', 54660),\n ('drink', 54650),\n ('pair', 54561),\n ('serious', 54560),\n ('pocket', 54555),\n ('missed', 54546),\n ('silver', 54519),\n ('disappeared', 54503),\n ('tightly', 54466),\n ('choice', 54340),\n ('hot', 54263),\n ('speaking', 54256),\n ('hide', 54171),\n ('low', 53930),\n ('expect', 53926),\n ('age', 53914),\n ('brown', 53830),\n ('ravenclaw', 53746),\n ('definitely', 53743),\n ('aware', 53732),\n ('eventually', 53575),\n ('spot', 53465),\n ('wake', 53169),\n ('putting', 53074),\n ('hug', 53034),\n ('bedroom', 52933),\n ('god', 52855),\n ('ears', 52832),\n ('single', 52795),\n ('simple', 52786),\n ('forest', 52750),\n ('killing', 52655),\n ('waved', 52619),\n ('mad', 52539),\n ('present', 52382),\n ('continue', 52348),\n ('laughter', 52325),\n ('scared', 52291),\n ('six', 52204),\n ('mirror', 52201),\n ('dress', 52181),\n ('ok', 52124),\n ('admit', 52055),\n ('sounds', 51934),\n ('exclaimed', 51904),\n ('aunt', 51891),\n ('feelings', 51854),\n ('corridor', 51840),\n ('cedric', 51829),\n ('join', 51824),\n ('paper', 51813),\n ('handed', 51625),\n ('blinked', 51520),\n ('murmured', 51470),\n ('showed', 51396),\n ('slipped', 51186),\n ('check', 51158),\n ('breathing', 51132),\n ('relief', 50965),\n ('forehead', 50935),\n ('box', 50823),\n ('force', 50728),\n ('sleeping', 50720),\n ('doors', 50684),\n ('younger', 50662),\n ('smirk', 50616),\n ('final', 50609),\n ('writing', 50502),\n ('news', 50463),\n ('wands', 50438),\n ('causing', 50364),\n ('protect', 50149),\n ('bathroom', 50141),\n ('indeed', 49734),\n ('steps', 49516),\n ('ball', 49498),\n ('kissing', 49455),\n ('dangerous', 49391),\n ('wards', 49211),\n ('alley', 49034),\n ('lunch', 48991),\n ('glance', 48980),\n ('obvious', 48830),\n ('cause', 48826),\n ('entrance', 48739),\n ('stayed', 48675),\n ('business', 48665),\n ('seamus', 48623),\n ('married', 48584),\n ('dean', 48578),\n ('visit', 48576),\n ('sudden', 48569),\n ('party', 48531),\n ('touched', 48514),\n ('needs', 48510),\n ('fuck', 48502),\n ('evil', 48492),\n ('difficult', 48455),\n ('drew', 48447),\n ('apart', 48438),\n ('busy', 48400),\n ('ring', 48256),\n ('honestly', 48217),\n ('imagine', 48206),\n ('stuff', 48130),\n ('fault', 48027),\n ('number', 47958),\n ('burst', 47884),\n ('aside', 47776),\n ('walls', 47770),\n ('leaning', 47762),\n ('groaned', 47560),\n ('crowd', 47508),\n ('dream', 47465),\n ('slytherins', 47465),\n ('interrupted', 47386),\n ('silently', 47364),\n ('snake', 47251),\n ('learned', 47241),\n ('couch', 47225),\n ('darkness', 47194),\n ('remained', 47111),\n ('hello', 47088),\n ('stated', 47052),\n ('tower', 47038),\n ('dragon', 46995),\n ('foot', 46960),\n ('odd', 46904),\n ('growled', 46618),\n ('teacher', 46607),\n ('spend', 46595),\n ('ahead', 46548),\n ('wanting', 46544),\n ('realised', 46413),\n ('allow', 46371),\n ('stuck', 46342),\n ('human', 46305),\n ('points', 46262),\n ('blonde', 46253),\n ('lose', 46151),\n ('grew', 46133),\n ('relationship', 46115),\n ('shop', 46068),\n ('lavender', 46023),\n ('match', 46001),\n ('date', 45964),\n ('finish', 45892),\n ('act', 45884),\n ('hiding', 45827),\n ('umbridge', 45795),\n ('caused', 45793),\n ('werewolf', 45785),\n ('response', 45583),\n ('crossed', 45562),\n ('tight', 45485)]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Топ-1000 слов по встречаемости:')\n",
    "sorted([(word, count) for word, count in word_counter.items()], key=lambda x: x[1], reverse=True)[:1000]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "2. Найдите топ-10 по частоте: имен, пар имя + фамилия, пар вида ''профессор'' + имя / фамилия."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "name_counter = defaultdict(lambda: 0)\n",
    "\n",
    "for text in tqdm(tokenized_texts):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    for name, ner_tag in nltk.ne_chunk(pos_tags):\n",
    "        if ner_tag == '':\n",
    "            pass  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "[бонус] Найдите еще что-то интересное в корпусе (что-то специфичное для фанфиков или фентези-тематики)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть 2. [2 балла] Модели представления слов\n",
    "Обучите модель представления слов (word2vec, GloVe, fastText или любую другую) на материале корпуса HPAC."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/germanarutunov/DataspellProjects/hw-1-nlp-hse-2022-AndBoyS/hw\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "split_data_dir = data_dir / 'hpac_splits'\n",
    "\n",
    "ft_data_dir = data_dir / 'ft'\n",
    "ft_data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "ft_train_file = ft_data_dir / 'hpac_ft.train'\n",
    "ft_dev_file = ft_data_dir / 'hpac_ft.dev'\n",
    "\n",
    "models_dir = Path('models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "ft_model = models_dir / 'ft.ftz'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "should_train_ft = True\n",
    "should_train_split_ft = True\n",
    "should_dev_split_ft = True\n",
    "\n",
    "if ft_train_file.exists():\n",
    "    should_train_split_ft = False\n",
    "\n",
    "if ft_dev_file.exists():\n",
    "    should_dev_split_ft = False\n",
    "\n",
    "if ft_model.exists():\n",
    "    should_train_ft = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "if should_train_split_ft:\n",
    "    train_df = pd.read_csv(split_data_dir / 'hpac_training_128.tsv',\n",
    "                           names=['target', 'text'],\n",
    "                           index_col=0,\n",
    "                           sep='\\t', header=None)\n",
    "    with open(ft_train_file, mode='a') as f:\n",
    "        for i, (target, text) in tqdm(train_df.iterrows(), desc='Processing train split for fastText'):\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "            f.write(f'__label__{target}\\t{text}\\n')\n",
    "    should_train_split_ft = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "Processing dev split for fastText: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aed506615af24295aac487ceb36279eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if should_dev_split_ft:\n",
    "    dev_df = pd.read_csv(split_data_dir / 'hpac_dev_128.tsv',\n",
    "                         names=['target', 'text'],\n",
    "                         index_col=0,\n",
    "                         sep='\\t', header=None)\n",
    "    with open(ft_dev_file, mode='a') as f:\n",
    "        for i, (target, text) in tqdm(dev_df.iterrows(), desc='Processing dev split for fastText'):\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "            f.write(f'__label__{target}\\t{text}\\n')\n",
    "    should_dev_split_ft = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import fasttext"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100.0% Trials:    3 Best score:  0.260092 ETA:   0h 0m 0s 95.3% Trials:    3 Best score:  0.260092 ETA:   0h 0m56s\n",
      "Training again with best arguments\n",
      "Read 7M words\n",
      "Number of words:  58022\n",
      "Number of labels: 85\n",
      "Progress: 100.0% words/sec/thread:  427044 lr:  0.000000 avg.loss:  3.599567 ETA:   0h 0m 0s 58.2% words/sec/thread:  383426 lr:  0.320569 avg.loss:  3.709370 ETA:   0h 0m 2s\n",
      "Progress: 100.0% words/sec/thread:  459367 lr:  0.000000 avg.loss:  3.411367 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "if should_train_ft:\n",
    "    model = fasttext.train_supervised(\n",
    "        input=str(ft_train_file),\n",
    "        autotuneValidationFile=str(ft_dev_file),\n",
    "        autotuneModelSize='5M',\n",
    "        autotuneDuration=1200\n",
    "    )\n",
    "    model.save_model(str(ft_model))\n",
    "    should_train_ft = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Продемонстрируйте, как работает поиск синонимов, ассоциаций, лишних слов в обученной модели."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.1 Синонимы"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.8944582343101501, 'rescue'),\n (0.888363242149353, 'ridiculously'),\n (0.8825259208679199, 'jeremiah'),\n (0.8562796115875244, 'witnessed'),\n (0.8521283864974976, 'hounding'),\n (0.8432715535163879, 'none'),\n (0.8367645740509033, 'hints'),\n (0.8300632834434509, 'weaved'),\n (0.82944256067276, 'sacred'),\n (0.8291577100753784, 'betas')]"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_nearest_neighbors('wizard')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.2 Ассоциации"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "[(0.9383688569068909, 'combats'),\n (0.9187878966331482, 'mocked'),\n (0.9064145088195801, 'ooooo'),\n (0.9061028361320496, 'force'),\n (0.903786301612854, 'roared'),\n (0.9032526612281799, 'jezlyn'),\n (0.8954090476036072, 'wheezley'),\n (0.8932932019233704, 'abstract'),\n (0.8926324844360352, 'fenir'),\n (0.8868676424026489, 'caged')]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_analogies('zombie', 'monster', 'beast')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1.3 Лишние слова"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Не знаю, как это тут сделать"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Визуализируйте топ-1000 слов по частоте без учета стоп-слов (п. 1.1) с помощью TSNE или UMAP (https://umap-learn.readthedocs.io)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Нужны предыдущие пункты"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Часть 3. [5 баллов] Классификация текстов\n",
    "Задача классификации формулируется так: данный фрагмент фанфика описывают какую-то ситуацию, которая предшествует произнесению заклинания. Требуется по тексту предсказать, какое именно заклинание будет произнесено. Таким образом, заклинание - это фактически метка класса. Основная мера качества – macro $F_1$.\n",
    "Обучите несколько классификаторов и сравните их между собой. Оцените качество классификаторов на частых и редких классах. Какие классы чаще всего оказываются перепутаны? Связаны ли ошибки со смыслом заклинаний?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 fastText"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "ft_data_dir = data_dir / 'ft'\n",
    "\n",
    "ft_test_file = ft_data_dir / 'hpac_ft.test'\n",
    "\n",
    "models_dir = Path('models')\n",
    "ft_model = models_dir / 'ft.ftz'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "should_test_split_ft = True\n",
    "\n",
    "if ft_test_file.exists():\n",
    "    should_test_split_ft = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "if should_test_split_ft:\n",
    "    test_df = pd.read_csv(split_data_dir / 'hpac_test_128.tsv',\n",
    "                           names=['target', 'text'],\n",
    "                           index_col=0,\n",
    "                           sep='\\t', header=None)\n",
    "    with open(ft_test_file, mode='a') as f:\n",
    "        for i, (target, text) in tqdm(test_df.iterrows(), desc='Processing test split for fastText'):\n",
    "            text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "            f.write(f'__label__{target}\\t{text}\\n')\n",
    "    should_test_split_ft = False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.load_model(str(ft_model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "scores = model.test_label(str(ft_test_file))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 for fastText is 0.3122\n"
     ]
    }
   ],
   "source": [
    "macro_f1 = []\n",
    "\n",
    "for label, metrics in scores.items():\n",
    "    if np.isnan(metrics['f1score']):\n",
    "        continue\n",
    "    macro_f1 = metrics['f1score']\n",
    "\n",
    "macro_f1 = np.mean(macro_f1)\n",
    "print(f'Macro F1 for fastText is {macro_f1:.4f}')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
